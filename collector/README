Reference Collector README




1. Packages to install

   * IF using postgres
      (1) sudo apt-get install postgresql python-psycopg2
   * IF using mysql
      (1) sudo apt-get install mysql-server mysql-client python-mysqldb

    (2) Install the python requests package.
        (a) sudo apt-get install python-dev python-pip 
        (b) sudo pip install requests
        (c) If requests was installed through apt-get install python-requests,
            upgrade it through pip:
            sudo pip install requests --upgrade

2a. PostgreSQL Configuration

   We use <COLLECTOR_OPERATOR_NAME>=agguser, but any name you like
   should work.

    Omit (1) if you already have a postgres user
    (1) sudo -u postgres psql -c '\password postgres' postgres
    (2) sudo -u postgres createuser --no-createdb --no-createrole \
          --no-superuser <COLLECTOR_OPERATOR_NAME>
    (3) sudo -u postgres psql -c '\password <COLLECTOR_OPERATOR_NAME>' postgres
    (4) sudo -u postgres createdb --owner=<COLLECTOR_OPERATOR_NAME> collector

2b. MySQL Configuration

    (1) echo "CREATE DATABASE collector" | mysql -u root -p
    (2) echo "GRANT ALL ON collector.* TO <COLLECTOR_OPERATOR_NAME>@localhost IDENTIFIED BY '<COLLECTOR_PASSWORD>'" | mysql -u root -p

3. Install and configure ops-monitoring software

   This puts the software in /usr/local/ops-monitoring.  You can put it
   elsewhere, but may need to change other configuration down the line
   to point to the correct paths.

    (1) cd /usr/local
    (2) sudo tar xvjf /path/to/ops-monitoring.tar.bz2
    (3) sudo $EDITOR ops-monitoring/config/collector_datastore_operator.conf
        * set username: <COLLECTOR_OPERATOR_NAME>
        * set password: <psql password for <COLLECTOR_OPERATOR_NAME> you set above>
        * IF you are using mysql, set dbtype: mysql in the [main] section
    (4) Ensure that ownership of the ops-monitoring/ directory is root:root
        chown -R root /usr/local/ops-monitoring
        chgrp -R root /usr/local/ops-monitoring


4. Configuring the collector to point to local datastore(s)

    (1) Attempt to reach the tamassos config datastore. Use the unit-tests
    program single_rest_call.py and point to

    cd unit-tests
    python single_rest_call.py -u https://tamassos.gpolab.bbn.com/info/opsconfig/geni-prod -c /path/to/collector/certificate

    See unit-tests/README for more detail

    (2) The output of this config store will list the URLs of the aggregate
    and external check datastores


5. Collector programs:

   The datastores_dict item in '/config/collector_operator.conf' is
   the collection of local datastores to be queried.  Use these to
   form arguments in the execution of the collector programs.

   (1) Apply for a tool certificate as described here:
   http://groups.geni.net/geni/wiki/OperationalMonitoring/DatastorePolling#Security
   You only need to do this once.

   (2) single_local_datastore_info_crawler.py performs information
   fetching. It puts data in the collector database.  It needs to be
   called with four arguments:

     -c or --certpath <pathname> provides the path to your tool certificate
     -b or --baseurl with <url_of_info_url> 
        This starts with "https://" and the convention is to end with "/info"
     -a or --aggregateid with aggregate_id at -b's datastore (i.e., gpo-ig)
     -o or --object-types with letters of object types to get info on
     	n: for node
	i: for interface
	s: for sliver
	l: for link
	v: for vlan
	So argument -o nislv will get information on all object types
     -d or --debug will print what would be added to the collector database
        to the screen and does not add anything to the database
     -h or --help prints the usage() function, repeat of this info
   
   
   (3) single_local_datastore_object_type_fetcher.py does a single fetch
   of timeseries data. It needs to be called with three arguments:

     -c or --certpath <pathname> provides the path to your tool certificate
     -a or --aggregateid of the aggregate in the ops_aggregate table
     -o or --object-type of object to get (note singularity). Valid object 
        types are (only one is passed, unlike the info_crawler above):
	n: for node
	i: for interface
	v: for vlan
	So argument -o n will get ts data for all node events and all nodes
	at the datastore
     -d or --debug will print what would be added to the collector database
        to the screen and does not add anything to the database
     -h or --help prints the usage() function, repeat of this info

   (4) Running these programs as cron jobs.

    Go into crontab:

    crontab -e

    Add the jobs you wish to run; examples are below.  In the first
    example, the info crawler is run once per day at 11:30pm.  It
    fetches objects of type n and i (-o ni).  Substitute the
    aggregate_id and url of the datastore you wish to aggregate data
    from.
    
    In the last two examples, the data fetchers nodes (-o n) and
    interfaces (-o i) runs every 5 minutes.  Substitute the
    aggregate_id the same as for the info crawler. Save and exit
    crontab.

    30 23 * * * (cd /usr/local/ops-monitoring/collector; python single_local_datastore_info_crawler.py -c /path/to/cert/collector-xyz.pem -b http://tamassos.gpolab.bbn.com/info/ -a gpo-ig -o ni)

    */5 * * * * (cd /usr/local/ops-monitoring/collector; python single_local_datastore_object_type_fetcher.py -c /path/to/cert/collector-xyz.pem -a gpo-ig -o n)

    */5 * * * * (cd /usr/local/ops-monitoring/collector; python single_local_datastore_object_type_fetcher.py -c /path/to/cert/collector-xyz.pem -a gpo-ig -o i)

6. Query Library

   The query library is used by services that sit on top of the collector.
   The query library is currently contained in the collector module under
   the file psql_queries.py.  This file should be installed under 
   /usr/local/lib/ at this time.
